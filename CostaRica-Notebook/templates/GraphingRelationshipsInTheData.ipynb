{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphically Exploring your data \n",
    "In this notebook we will look at ways to graphically depict your data and explore various relationships that might exist. Before running this notebook please save out the plot_data.csv and plot_subplot.csv files from the Summarizing_plot_data.ipynb notebooks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the plot_subplot.csv files into a dataframe, separate response and predictor variables, and describe the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sub_df= pd.read_csv('plot_subplot_data.csv')\n",
    "plot_sub_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=['plotid', 'sampleid', ]\n",
    "\n",
    "resp=['Use', 'Cover', 'Vegetation',\n",
    "       'Herbaceous', 'Grass', 'Cultivation', 'WetLand', 'Terrain', 'Water',\n",
    "       'Another Class', 'SAF',]\n",
    "\n",
    "pred=['BLUE',\n",
    "       'GREEN', 'NIR', 'RED', 'SWIR1', 'SWIR2', 'altura2', 'aspect',\n",
    "       'aspectcos', 'aspectdeg', 'aspectYesn', 'brightness', 'clay_1mMed',\n",
    "       'diff', 'elevation', 'evi', 'fpar', 'hand30_100', 'lai', 'mTPI', 'ndvi',\n",
    "       'ocs_1mMed', 'sand_1mMed', 'savi', 'Yeslt_1mMed', 'slope', 'topDiv',\n",
    "       'wetness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sub_df[resp].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sub_df[pred].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note, that with the plot subplot dataset we have over 100K observations. To explore general trends in the data we don't necessarily need to use all the data. To reduce the amount of processing and speed up analyses we can take a random sample of the data. Let's random choose 1% of the observations to look for trends.\n",
    "\n",
    "There are multiple ways to randomly choose observations from a dataframe. Here we are going to use numpy and the choice function to randomly select row indices and further use those selected index values to select records from the plots_subplots dataframe.\n",
    "- Sample the plot_subplot data n=1% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N=plot_sub_df.shape[0]\n",
    "n=int(N*0.01)\n",
    "ind=np.random.choice(N,n,replace=False) #randomly choosing 1% of the index values without replacement\n",
    "sub_df=plot_sub_df.iloc[ind]#using panda's iloc (index slicing) function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plot matrix, correlation matrix, and box plots of predictor variables for a random subset of the data.\n",
    "- Make a scatter plot matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(sub_df[pred], alpha=0.2, figsize=(15, 15),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df[pred].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- boxplots; due to the number of box plots, we will use matplotlib directly to create subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,ax=plt.subplots(5,6,figsize=(15,15)) #make place holder for 30 box plots. Remember, we have 28 columns.\n",
    "cnt=0\n",
    "for nm in pred:\n",
    "    r=cnt%5\n",
    "    c=cnt//5\n",
    "    sub_df[[nm]].boxplot(ax=ax[r,c])\n",
    "    cnt+=1\n",
    "\n",
    "ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's look at the response variables \n",
    "### These are categorical variables so we have fewer ways to graphically display aspects of the data. In this instance we will create a grid of pie charts depicting the proportion of each category within a given response variable.\n",
    "\n",
    "- pie charts for 11 response variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(4,3,figsize=(15,15)) #make place holder for 12 pie charts. Remember, we have 11 response columns.\n",
    "cnt=0\n",
    "for nm in resp:\n",
    "    r=cnt%4\n",
    "    c=cnt//4\n",
    "    sub_df[[nm]].value_counts().plot(ax=ax[r,c],kind='pie',title=nm,autopct='%1.1f%%')\n",
    "    cnt+=1\n",
    "\n",
    "ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Interpretation\n",
    "- How many unique categories are in the Use column?\n",
    "- What is the average lai value?\n",
    "- What does the scatter plot matrix tell us?\n",
    "- What does the correlation matrix tell us?\n",
    "- What insights can we glen from the boxplots?\n",
    "- What insights can we glen form the pie charts?\n",
    "- Why did we ue a random sample of the plot subplot data?\n",
    "- Task; explore general trends in the summarized plot data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the scatter plot matrix and correlation matrix note the amount of linear correlation among the predictor variables. Let's perform a PCA using all the data and transform our data into independent components.\n",
    "\n",
    "We wil be using sklearn to [scale](https://scikit-learn.org/stable/modules/preprocessing.html) and perform a [principal component analysis](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA) (PCA). If you want to learn more about PCAs check out sklearn's [user guide](https://scikit-learn.org/stable/modules/decomposition.html) and [examples](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html#sphx-glr-auto-examples-decomposition-plot-pca-iris-py). Let's create a PCA and look at some of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss=StandardScaler(with_mean=False) #scaling data without centering on the mean because PCA will center the data.\n",
    "X=plot_sub_df[pred].values\n",
    "ss.fit(X)\n",
    "X2=ss.transform(X) # scaling our values so they are comparable\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X2) # fit the PCA on scaled values\n",
    "vexp=pd.DataFrame(pca.explained_variance_ratio_,columns=['var']) # get the proportion of variance explained by each component\n",
    "\n",
    "#find the number of components needed to account for 95% of the variation in the data\n",
    "cmp=0\n",
    "s=0\n",
    "for v in vexp['var']:\n",
    "    s+=v\n",
    "    cmp+=1\n",
    "    if(s>0.95):break\n",
    "    \n",
    "#plot % covariance explained\n",
    "print('95% of the covariation can be explained using the first',cmp,'components')\n",
    "vexp.plot(kind='barh',figsize=(15,8),title='Percent variation by each component').invert_yaxis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying the % of variance explained by each component in this manner highlights that there was substantial linear correlation among predictor variables and gives us a graphical way to select how many components we want to keep in future analyses. In this case, we are looking to keep the minimum number of component that account for 95% of the variation(information) in the data. In the horizontal bar chart it appears that the accumulative amount of variation explained by adding the next component, levels off around the 14th component. Setting a threshold of 95% of the variation indicates we need to keep 13 components to account for 95% of the variation, which agrees with our visual interpretation of the horizontal bar chart. So let's transform our Google Earth Engine data into 13 independent components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the predictor values to 13 components and plot a scatter plot matrix with Use labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_df=pd.DataFrame(pca.transform(X2)[:,0:13]) #just keep the first 13 components and turn them into a dataframe\n",
    "cmp_df['Use']=plot_sub_df['Use'] # add the use response variable to the dataframe\n",
    "display(cmp_df)\n",
    "uvls=cmp_df['Use'].value_counts()/cmp_df.shape[0]\n",
    "print('Proportion of Use categories')\n",
    "uvls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do the proportion match the pie chart? Why or why not?\n",
    "- scatter plot using components Use labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a color dictionary for each category\n",
    "color_dic={uvls.index[0]:'green',uvls.index[1]:'tan',uvls.index[2]:'yellow',uvls.index[3]:'purple',uvls.index[4]:'blue',uvls.index[5]:'brown',uvls.index[6]:'grey',uvls.index[7]:'grey'}\n",
    "cmp_df['color']=cmp_df['Use'].map(color_dic)\n",
    "tdf=cmp_df.iloc[ind]\n",
    "\n",
    "#pd.plotting.scatter_matrix(tdf.iloc[:,0:-2])\n",
    "pd.plotting.scatter_matrix(tdf[np.arange(13)],c=tdf['color'],figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the fist and second components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_df.plot(kind='scatter',x=0,y=1,c='color',figsize=(15,15),xlabel='Comp1',ylabel='Comp2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Interpretation 2\n",
    "- What proportion of the variance is explained by the second principal component?\n",
    "- Why did we scale our data before performing PCA?\n",
    "- What do the colors mean in the scatter plot matrix and the scatter plot of Comp1 and Comp2?\n",
    "- What does component 1 and 2 mean?\n",
    "- Does it look like various categories group together?\n",
    "- Do you see any extreme values?\n",
    "- Task: apply some of these same visualization approaches with the summarized plot data. Remember that the response variables are no longer categorical. We converted them to continuous variables (%).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme points and potential outliers\n",
    "### To identify potential extreme values we will be using [Isolation_Forest](https://scikit-learn.org/stable/auto_examples/ensemble/plot_isolation_forest.html#sphx-glr-auto-examples-ensemble-plot-isolation-forest-py). Isolation Forest identifies extreme values by, \"randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature\". Extreme observations are labeled with a value of -1 while inner values are labeled with a 1. Additionally, anomaly distance can be calculated to rank extreme values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "#set the X values\n",
    "X=cmp_df[cmp_df.columns[:13]]\n",
    "\n",
    "isf = IsolationForest(max_samples=0.75, random_state=0)\n",
    "isf.fit(X)\n",
    "\n",
    "lbl=isf.predict(X) #1 is inner -1 is outer value\n",
    "dist=isf.decision_function(X)\n",
    "color_dic={1:'green',-1:'red'}\n",
    "cmp_df['extreme']=lbl\n",
    "cmp_df['an_dist']=dist\n",
    "color=cmp_df['extreme'].map(color_dic)\n",
    "cmp_df.plot(kind='scatter',x=0,y=1,c=color,figsize=(15,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Interpretation 3\n",
    "- How many observation were labeled extreme?\n",
    "- Interpret the meaning of the 0 by 1 scatter plot? What do the green and red points mean?\n",
    "- What threshold was used to identify extreme values? Can you change that threshold?\n",
    "- Are extreme values outliers?\n",
    "- Should extreme value be removed from the analysis?\n",
    "- How can you check if extreme values are outliers?\n",
    "- If we decide to remove extreme values, how will that impact the summarized plot data?\n",
    "- Task1: Create a graph that highlights anomaly distance.\n",
    "- Task2: identify extreme values for the summarized plot data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rstools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
